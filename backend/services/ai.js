import crypto from 'crypto';
import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { hasEnoughCredits, deductCredits, CREDIT_COSTS } from './credits.js';
import { adminAnomaliesService } from './adminAnomalies.js';
import { debugIngest } from '../utils/debugIngest.js';
import { AI_CONFIG, getModelName, getHistoryLimit, getGenerationConfig } from '../config/ai.config.js';
import { aiLogger } from '../utils/logger.js';
import { geminiCircuitBreaker, openaiCircuitBreaker, openrouterCircuitBreaker } from '../utils/circuitBreaker.js';
import { countTokensSync } from '../utils/tokenizer.js';
import { getSmartWindow } from '../utils/conversationMemory.js';

/**
 * Service IA unifi√© - G√®re Gemini, OpenAI et OpenRouter
 * Int√®gre le syst√®me de cr√©dits pour la facturation
 */
class AIService {
    constructor() {
        this.openaiClient = null;
        this.geminiClient = null;
        this.openrouterClient = null;
        this.initialized = false;
    }

    /**
     * Initialise les clients IA au d√©marrage
     */
    initialize() {
        if (this.initialized) return;

        // Initialize Gemini (prioritaire)
        if (process.env.GEMINI_API_KEY) {
            try {
                this.geminiClient = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
                console.log('‚úÖ [AI] Google Gemini initialis√©');
            } catch (error) {
                console.error('‚ùå [AI] Erreur initialisation Gemini:', error.message);
            }
        }

        // Initialize OpenAI (fallback)
        if (process.env.OPENAI_API_KEY && process.env.OPENAI_API_KEY !== 'sk-your-openai-api-key') {
            try {
                this.openaiClient = new OpenAI({
                    apiKey: process.env.OPENAI_API_KEY
                });
                console.log('‚úÖ [AI] OpenAI initialis√©');
            } catch (error) {
                console.error('‚ùå [AI] Erreur initialisation OpenAI:', error.message);
            }
        }

        // Initialize OpenRouter (multi-mod√®les)
        if (process.env.OPENROUTER_API_KEY) {
            try {
                this.openrouterClient = new OpenAI({
                    apiKey: process.env.OPENROUTER_API_KEY,
                    baseURL: 'https://openrouter.ai/api/v1'
                });
                console.log('‚úÖ [AI] OpenRouter initialis√©');
            } catch (error) {
                console.error('‚ùå [AI] Erreur initialisation OpenRouter:', error.message);
            }
        }

        if (!this.geminiClient && !this.openaiClient && !this.openrouterClient) {
            console.warn('‚ö†Ô∏è [AI] Aucun service IA configur√© - r√©ponses de fallback activ√©es');
        }

        this.initialized = true;
    }

    /**
     * D√©termine le meilleur provider selon le mod√®le et la disponibilit√©
     */
    getProvider(modelName) {
        // If only OpenRouter is configured, route everything there
        if (this.openrouterClient && !this.geminiClient && !this.openaiClient) {
            return 'openrouter';
        }

        // Mod√®le OpenRouter (contient "/" ou ":free")
        if (modelName?.includes('/') || modelName?.includes(':')) {
            if (this.openrouterClient) return 'openrouter';
        }

        // Mod√®les explicitement Gemini (avant la r√®gle "contient /" pour OpenRouter)
        if (modelName?.startsWith('gemini') || modelName?.startsWith('models/gemini')) {
            if (this.geminiClient) return 'gemini';
            if (this.openrouterClient) return 'openrouter';
            if (this.openaiClient) return 'openai';
        }
        // Mod√®les explicitement OpenAI
        if (modelName?.startsWith('gpt')) {
            if (this.openaiClient) return 'openai';
            if (this.openrouterClient) return 'openrouter';
            if (this.geminiClient) return 'gemini';
        }
        // Par d√©faut: Gemini > OpenAI > OpenRouter > Fallback
        if (this.geminiClient) return 'gemini';
        if (this.openaiClient) return 'openai';
        if (this.openrouterClient) return 'openrouter';
        return 'fallback';
    }

    /**
     * G√©n√®re une r√©ponse IA avec gestion des cr√©dits et contexte enrichi
     * @param {Object} agent - Agent configuration
     * @param {Array} conversationHistory - Previous messages
     * @param {Object|string} normalizedPayloadOrMessage - Normalized payload { tenant_id, conversation_id, from, message, timestamp } or raw user message (legacy)
     * @param {Array} [knowledgeBaseOrUserId] - Knowledge base items (new) or userId (legacy 6-arg)
     * @param {Object|string} [messageAnalysisOrKnowledge] - Pre-analysis (new) or knowledge (legacy)
     * @param {Object} [messageAnalysisLegacy] - Pre-analysis (legacy 6-arg only)
     */
    async generateResponse(agent, conversationHistory, normalizedPayloadOrMessage, knowledgeBaseOrUserId = [], messageAnalysisOrKnowledge = null, messageAnalysisLegacy = null) {
        this.initialize();

        // Input validation
        if (!agent || typeof agent !== 'object') {
            throw new Error('[AI] Invalid agent: agent must be a valid object');
        }
        if (!agent.id) {
            console.warn('[AI] Warning: agent.id is missing');
        }
        if (!conversationHistory || !Array.isArray(conversationHistory)) {
            throw new Error('[AI] Invalid conversationHistory: must be an array');
        }
        if (!normalizedPayloadOrMessage) {
            throw new Error('[AI] Invalid message: normalizedPayloadOrMessage is required');
        }

        const isPayload = normalizedPayloadOrMessage && typeof normalizedPayloadOrMessage === 'object' && 'message' in normalizedPayloadOrMessage;
        let userMessage, userId, knowledgeBase, messageAnalysis;
        if (isPayload) {
            userMessage = normalizedPayloadOrMessage.message;
            userId = normalizedPayloadOrMessage.tenant_id ?? null;
            knowledgeBase = Array.isArray(knowledgeBaseOrUserId) ? knowledgeBaseOrUserId : [];
            messageAnalysis = messageAnalysisOrKnowledge;
        } else {
            userMessage = normalizedPayloadOrMessage;
            knowledgeBase = Array.isArray(knowledgeBaseOrUserId) ? knowledgeBaseOrUserId : [];
            if (typeof messageAnalysisOrKnowledge === 'string' || typeof messageAnalysisOrKnowledge === 'number') {
                userId = messageAnalysisOrKnowledge;
                messageAnalysis = messageAnalysisLegacy ?? null;
            } else {
                userId = null;
                messageAnalysis = messageAnalysisOrKnowledge;
            }
        }

        const provider = this.getProvider(agent.model);
        const model = agent.model || 'gemini-1.5-flash';

        const intent = messageAnalysis?.intent?.primary || 'unknown';
        console.log(`[AI] Provider: ${provider} | Model: ${model} | Intent: ${intent} | Message: "${userMessage.substring(0, 30)}..."`);

        // Check credits if userId is provided
        if (userId && provider !== 'fallback') {
            if (!hasEnoughCredits(userId, 'ai_message')) {
                console.log(`[AI] User ${userId} has insufficient credits for ${model}`);
                // Return fallback response when out of credits
                const fallback = this.fallbackResponse(agent, userMessage);
                fallback.credit_warning = 'Cr√©dits insuffisants - r√©ponse de secours utilis√©e';
                return fallback;
            }
        }

        try {
            let response;
            
            switch (provider) {
                case 'gemini':
                    // Use circuit breaker for Gemini
                    response = await geminiCircuitBreaker.execute(async () => {
                        return await this.generateGeminiResponse(agent, conversationHistory, userMessage, knowledgeBase, messageAnalysis);
                    });
                    break;
                case 'openai':
                    // Use circuit breaker for OpenAI
                    response = await openaiCircuitBreaker.execute(async () => {
                        return await this.generateOpenAIResponse(agent, conversationHistory, userMessage, knowledgeBase, messageAnalysis);
                    });
                    break;
                case 'openrouter':
                    // Use circuit breaker for OpenRouter
                    response = await openrouterCircuitBreaker.execute(async () => {
                        return await this.generateOpenRouterResponse(agent, conversationHistory, userMessage, knowledgeBase, messageAnalysis);
                    });
                    break;
                default:
                    response = this.fallbackResponse(agent, userMessage);
            }

            // Deduct credits only for real AI responses (not fallback)
            if (userId && provider !== 'fallback') {
                const tokensUsed = Number.isFinite(response?.tokens) ? response.tokens : 0;
                const deduction = deductCredits(userId, 'ai_message', 1, {
                    agent_id: agent.id,
                    tokens: tokensUsed
                });
                response.credits_deducted = deduction.cost;
                response.credits_remaining = deduction.credits_remaining;
                if (!deduction.success) {
                    console.warn(`[AI] Credit deduction failed: ${deduction.error}`);
                }
            }

            return response;
            
        } catch (error) {
            // Log error with context
            if (error.circuitBreakerOpen) {
                aiLogger.warn(`Circuit breaker is open for ${provider}`, { provider, error: error.message });
            } else if (error.timeout) {
                aiLogger.error(`Request timeout for ${provider}`, { provider, error: error.message });
            } else {
                aiLogger.error(`Error from ${provider}`, { provider, error: error.message });
            }
            
            // Essayer d'autres providers en cas d'erreur (skip si circuit breaker est open)
            if (provider === 'gemini' && this.openaiClient && !error.circuitBreakerOpen) {
                console.log('[AI] Fallback vers OpenAI...');
                try {
                    const response = await openaiCircuitBreaker.execute(async () => {
                        return await this.generateOpenAIResponse(agent, conversationHistory, userMessage, knowledgeBase, messageAnalysis);
                    });
                    if (userId) {
                        const tokensUsed = Number.isFinite(response?.tokens) ? response.tokens : 0;
                        const deduction = deductCredits(userId, 'ai_message', 1, { agent_id: agent.id, tokens: tokensUsed });
                        response.credits_deducted = deduction.cost;
                        response.credits_remaining = deduction.credits_remaining;
                    }
                    return response;
                } catch (e) {
                    console.error('[AI] Fallback OpenAI aussi √©chou√©:', e.message);
                }
            } else if (provider === 'openai' && this.geminiClient && !error.circuitBreakerOpen) {
                console.log('[AI] Fallback vers Gemini...');
                try {
                    const response = await geminiCircuitBreaker.execute(async () => {
                        return await this.generateGeminiResponse(agent, conversationHistory, userMessage, knowledgeBase, messageAnalysis);
                    });
                    if (userId) {
                        const tokensUsed = Number.isFinite(response?.tokens) ? response.tokens : 0;
                        const deduction = deductCredits(userId, 'ai_message', 1, { agent_id: agent.id, tokens: tokensUsed });
                        response.credits_deducted = deduction.cost;
                        response.credits_remaining = deduction.credits_remaining;
                    }
                    return response;
                } catch (e) {
                    console.error('[AI] Fallback Gemini aussi √©chou√©:', e.message);
                }
            } else if (provider === 'openrouter' && !error.circuitBreakerOpen) {
                // OpenRouter a √©chou√©, essayer Gemini ou OpenAI
                if (this.geminiClient) {
                    console.log('[AI] OpenRouter √©chou√©, fallback vers Gemini...');
                    try {
                        const response = await geminiCircuitBreaker.execute(async () => {
                            return await this.generateGeminiResponse(agent, conversationHistory, userMessage, knowledgeBase, messageAnalysis);
                        });
                        if (userId) {
                            const tokensUsed = Number.isFinite(response?.tokens) ? response.tokens : 0;
                            const deduction = deductCredits(userId, 'ai_message', 1, { agent_id: agent.id, tokens: tokensUsed });
                            response.credits_deducted = deduction.cost;
                            response.credits_remaining = deduction.credits_remaining;
                        }
                        return response;
                    } catch (e) {
                        console.error('[AI] Fallback Gemini aussi √©chou√©:', e.message);
                    }
                }
                if (this.openaiClient) {
                    console.log('[AI] OpenRouter √©chou√©, fallback vers OpenAI...');
                    try {
                        const response = await openaiCircuitBreaker.execute(async () => {
                            return await this.generateOpenAIResponse(agent, conversationHistory, userMessage, knowledgeBase, messageAnalysis);
                        });
                        if (userId) {
                            const tokensUsed = Number.isFinite(response?.tokens) ? response.tokens : 0;
                            const deduction = deductCredits(userId, 'ai_message', 1, { agent_id: agent.id, tokens: tokensUsed });
                            response.credits_deducted = deduction.cost;
                            response.credits_remaining = deduction.credits_remaining;
                        }
                        return response;
                    } catch (e) {
                        console.error('[AI] Fallback OpenAI aussi √©chou√©:', e.message);
                    }
                }
            }

            // Fallback after all APIs failed: no credit deduction (fallback response is free)
            const fallback = this.fallbackResponse(agent, userMessage);
            console.log('[AI] Using fallback response (no credits deducted)');
            return fallback;
        }
    }

    /**
     * Construit le prompt syst√®me en blocs [SYSTEM GLOBAL], [BUSINESS TENANT], [POLICY].
     * Retourne { prompt, promptHash } pour versioning dans les logs.
     * @param {Object} agent - Agent configuration
     * @param {Array} knowledgeBase - Knowledge base items
     * @param {Object} messageAnalysis - Pre-analysis of the user message (optional)
     */
    /** Decode HTML entities in stored prompts so the LLM receives normal quotes/apostrophes */
    decodeHtmlEntities(str) {
        if (!str || typeof str !== 'string') return str;
        return str
            .replace(/&quot;/g, '"')
            .replace(/&#039;/g, "'")
            .replace(/&#39;/g, "'")
            .replace(/&amp;/g, '&')
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>');
    }

    /**
     * Validate and sanitize system prompt to prevent prompt injection
     * @param {string} prompt - The system prompt to validate
     * @returns {{ valid: boolean, sanitized: string, warnings: string[] }}
     */
    validateAndSanitizePrompt(prompt) {
        const warnings = [];
        
        if (!prompt || typeof prompt !== 'string') {
            return { valid: false, sanitized: '', warnings: ['Prompt is empty or invalid'] };
        }

        let sanitized = prompt.trim();

        // Size limit: 10000 characters
        const MAX_PROMPT_LENGTH = 10000;
        if (sanitized.length > MAX_PROMPT_LENGTH) {
            sanitized = sanitized.substring(0, MAX_PROMPT_LENGTH);
            warnings.push(`Prompt truncated to ${MAX_PROMPT_LENGTH} characters`);
        }

        // Detect potential prompt injection patterns
        const dangerousPatterns = [
            /ignore\s+(all\s+)?(previous|above|prior)\s+instructions?/gi,
            /disregard\s+(all\s+)?(previous|above|prior)\s+instructions?/gi,
            /forget\s+(all\s+)?(previous|above|prior)\s+instructions?/gi,
            /new\s+instructions?:/gi,
            /system\s*:\s*you\s+are\s+now/gi,
            /\[INST\]/gi,
            /\[\/INST\]/gi,
            /<\|im_start\|>/gi,
            /<\|im_end\|>/gi,
        ];

        for (const pattern of dangerousPatterns) {
            if (pattern.test(sanitized)) {
                warnings.push(`Potentially dangerous pattern detected: ${pattern.source}`);
            }
        }

        return {
            valid: warnings.length === 0,
            sanitized,
            warnings
        };
    }

    buildSystemPrompt(agent, knowledgeBase, messageAnalysis = null) {
        const hasCustomPrompt = agent.system_prompt && agent.system_prompt.trim().length > 0;

        // [SYSTEM GLOBAL] ‚Äî verrouill√©: pr√©sentation et contexte
        let systemGlobal;
        if (hasCustomPrompt) {
            const decoded = this.decodeHtmlEntities(agent.system_prompt);
            const validation = this.validateAndSanitizePrompt(decoded);
            
            if (validation.warnings.length > 0) {
                console.warn(`[AI] System prompt warnings for agent ${agent.id}:`, validation.warnings);
            }
            
            systemGlobal = validation.sanitized;
        } else {
            systemGlobal = this.getDefaultPrompt(agent);
        }
        if (!hasCustomPrompt) systemGlobal += this.getDefaultInstructions();
        systemGlobal += '\n\n‚ö†Ô∏è R√àGLE FINALE ‚Äî PR√âSENTATION: Ne dis JAMAIS "Je suis [nom]", "Je m\'appelle...", "votre assistant [type]..." ou toute phrase qui te pr√©sente. R√©ponds directement au message du client.';
        systemGlobal += '\n\n‚ö†Ô∏è R√àGLE ‚Äî CONTEXTE: Utilise la CONVERSATION R√âCENTE fournie. Si le client a d√©j√† √©t√© salu√© (√©change pr√©c√©dent avec "Bonjour" ou salut), NE REDIS PAS "Bonjour" ni "Bonjour !" au d√©but de ta r√©ponse. R√©ponds directement √† sa question ou demande (ex: produit, commande). Tu ne salues qu\'une seule fois au tout premier message du client.';
        systemGlobal += '\n\n‚ö†Ô∏è R√àGLE ‚Äî FORMULATION: Quand tu transmets des informations du catalogue ou de la base de connaissances, utilise des formulations professionnelles comme "Voici les informations disponibles", "D\'apr√®s notre catalogue, ...", "Voici ce qui est indiqu√© : ...". Ne dis JAMAIS "C\'est tout ce que j\'ai comme information", "Je n\'ai que √ßa", ou des formulations qui sous-entendent un manque. Reste factuel et rassurant.';

        // [BUSINESS TENANT] ‚Äî agent/catalogue et contexte temps r√©el
        let businessTenant = '';
        if (messageAnalysis) businessTenant += '\n\n' + this.buildAnalysisContext(messageAnalysis);
        if (knowledgeBase && knowledgeBase.length > 0) {
            businessTenant += '\n\nüìö BASE DE CONNAISSANCES:\n';
            for (const item of knowledgeBase) {
                const content = item.content.length > 2000 ? item.content.substring(0, 2000) + '...' : item.content;
                businessTenant += `### ${item.title}\n${content}\n\n`;
            }
            const hasCatalogue = knowledgeBase.some(item => item.title === 'üì¶ CATALOGUE PRODUITS');
            if (hasCatalogue) {
                    businessTenant += '\n‚ö†Ô∏è R√àGLE ‚Äî CATALOGUE: Pour les noms de produits, prix, disponibilit√©, description, caract√©ristiques et capacit√©s techniques, utilise UNIQUEMENT la section "üì¶ CATALOGUE PRODUITS" ci-dessus. Chaque produit peut avoir une description sur la ligne suivante (indentation) : utilise-la pour r√©pondre aux questions sur les capacit√©s, sp√©cifications ou fiche technique. N\'invente aucune caract√©ristique. Si le client demande les capacit√©s/caract√©ristiques d\'un produit et qu\'aucune description n\'est indiqu√©e pour ce produit dans le catalogue, dis-le clairement et propose de le mettre en relation avec un conseiller pour les d√©tails techniques.\n';
                    businessTenant += '‚ö†Ô∏è IMAGES PRODUITS: Si le client demande une image/photo d\'un produit, fournis le lien pr√©sent dans le catalogue (champ "Image: <url>") et n\'invente jamais de lien. Si le produit n\'a pas de lien d\'image, dis-le simplement.\n';
            }
        }

        // [POLICY] ‚Äî l√©gal/s√©curit√©
        const policy = '\n\n[POLICY] ‚Äî R√àGLES L√âGALES / S√âCURIT√â: Ne fais jamais de promesse de garantie absolue, ni de promesse de d√©lai non autoris√©e. N\'invente aucun prix ni information hors catalogue.\n\n‚ö†Ô∏è QUAND DEMANDER UN CONSEILLER (need_human: true):\n- R√©clamation grave ou litige complexe\n- Probl√®me technique critique n√©cessitant intervention\n- Informations de paiement ou donn√©es bancaires\n\n‚úÖ TU PEUX R√âPONDRE DIRECTEMENT (need_human: false):\n- Questions sur produits, prix, disponibilit√©, capacit√©s/caract√©ristiques/description (utilise le catalogue et la description produit)\n- Demandes de retour/remboursement (utilise la politique de retour dans la base de connaissance)\n- Questions sur livraison, d√©lais, zones desservies\n- Informations g√©n√©rales pr√©sentes dans ta base de connaissance';

        const fullPrompt = `[SYSTEM GLOBAL]\n${systemGlobal}\n\n[BUSINESS TENANT]\n${businessTenant}\n${policy}`;
        const promptHash = crypto.createHash('sha256').update(fullPrompt).digest('hex').slice(0, 16);
        return { prompt: fullPrompt, promptHash };
    }

    /**
     * Get default prompt for agent without custom prompt
     */
    getDefaultPrompt(agent) {
        return `Tu es un assistant commercial professionnel pour une boutique. Tu aides les clients avec leurs achats.

## ‚ö†Ô∏è PR√âSENTATION ‚Äî R√àGLE CRITIQUE
- NE JAMAIS te pr√©senter en disant ton nom ou ton r√¥le (ex: "Je suis [nom], votre assistant...", "Je m'appelle..."). Le client sait qu'il parle √† un assistant.
- En cas de salut ("Bonjour", "Salut"), r√©ponds par un salut court puis propose ton aide. Ex: "Bonjour ! Comment puis-je vous aider ?"
- Va TOUJOURS droit au but.

## R√àGLES FONDAMENTALES
- R√©ponds TOUJOURS dans la langue du client
- Sois CONCIS: 2-3 phrases maximum
- Va DROIT AU BUT, pas de formules longues
- Utilise les informations de contexte fournies
- Ne JAMAIS inventer d'informations`;
    }

    /**
     * Get default instructions
     */
    getDefaultInstructions() {
        return `\n\nüìã INSTRUCTIONS IMPORTANTES:
- R√©ponds dans la langue de l'utilisateur
- Maximum 2-3 phrases pour les r√©ponses simples
- Si tu ne sais pas, dis-le honn√™tement
- Utilise le contexte et la base de connaissances
- Ne r√©p√®te pas les informations d√©j√† donn√©es`;
    }

    /**
     * Build analysis context string for AI
     */
    buildAnalysisContext(analysis) {
        if (!analysis) return '';
        
        const parts = ['üîç CONTEXTE TEMPS R√âEL:'];
        
        // Language: force reply in the same language as the client
        if (analysis.language && analysis.language !== 'unknown') {
            const langLabel = analysis.language === 'fr' ? 'fran√ßais' : analysis.language === 'en' ? 'anglais' : analysis.language;
            parts.push(`\nüåê Langue du message client : ${langLabel}. R√©ponds UNIQUEMENT dans cette langue.`);
        }
        
        // Intent
        if (analysis.intent) {
            const intentLabels = {
                order: 'COMMANDE',
                inquiry: 'DEMANDE D\'INFO',
                complaint: 'R√âCLAMATION',
                greeting: 'SALUTATION',
                delivery_info: 'INFO LIVRAISON',
                human_request: 'DEMANDE HUMAIN',
                general: 'G√âN√âRAL'
            };
            parts.push(`Intention: ${intentLabels[analysis.intent.primary] || analysis.intent.primary}`);
        }
        
        // Products with real-time stock
        if (analysis.products?.matchedProducts?.length > 0) {
            parts.push('\nProduits mentionn√©s:');
            for (const p of analysis.products.matchedProducts) {
                const status = p.stockStatus === 'available' ? '‚úÖ Disponible' :
                              p.stockStatus === 'low' ? '‚ö†Ô∏è Stock limit√©' :
                              p.stockStatus === 'insufficient' ? '‚ö†Ô∏è Stock insuffisant' : '‚õî Rupture';
                parts.push(`- ${p.name}: ${p.price} FCFA | ${status} (${p.stock} en stock) | Qt√© demand√©e: ${p.requestedQuantity}`);
            }
        }
        
        // Stock issues - CRITICAL INFO
        if (analysis.products?.stockIssues?.length > 0) {
            parts.push('\n‚ö†Ô∏è ALERTES STOCK:');
            for (const issue of analysis.products.stockIssues) {
                parts.push(`- ${issue.message}`);
            }
            parts.push('‚Üí INFORME le client de ces probl√®mes de stock!');
        }
        
        // Customer history
        if (analysis.customerHistory) {
            const h = analysis.customerHistory;
            if (h.isRepeatCustomer) {
                parts.push(`\nüë§ Client fid√®le (${h.validatedOrders} commande(s), ${h.totalSpent} FCFA d√©pens√©s)`);
            } else if (h.isNewCustomer) {
                parts.push('\nüë§ Nouveau client');
            }
        }
        
        // Delivery info collected
        if (analysis.deliveryInfo?.hasDeliveryInfo) {
            parts.push('\nüìç Infos livraison d√©tect√©es:');
            if (analysis.deliveryInfo.city) parts.push(`- Ville: ${analysis.deliveryInfo.city}`);
            if (analysis.deliveryInfo.neighborhood) parts.push(`- Quartier: ${analysis.deliveryInfo.neighborhood}`);
            if (analysis.deliveryInfo.phone) parts.push(`- T√©l: ${analysis.deliveryInfo.phone}`);
        }
        
        // Order guidance
        if (analysis.isLikelyOrder) {
            const missing = [];
            if (!analysis.deliveryInfo?.city) missing.push('ville/commune');
            if (!analysis.deliveryInfo?.neighborhood) missing.push('quartier');
            if (!analysis.deliveryInfo?.phone) missing.push('num√©ro de t√©l√©phone');
            
            if (missing.length > 0) {
                parts.push(`\nüìù POUR FINALISER LA COMMANDE, demande: ${missing.join(', ')}`);
            } else {
                parts.push('\n‚úÖ Toutes les infos de livraison sont collect√©es!');
            }
        }
        
        // Support hints
        if (analysis.support?.ticketIntent) {
            parts.push('\nüõ†Ô∏è SUPPORT:');
            if (analysis.support.category) parts.push(`- Cat√©gorie: ${analysis.support.category}`);
            if (analysis.support.urgency) parts.push(`- Urgence: ${analysis.support.urgency}`);
        }

        // FAQ hints
        if (analysis.faq?.category && analysis.faq.category !== 'other') {
            parts.push('\n‚ùì FAQ:');
            parts.push(`- Cat√©gorie: ${analysis.faq.category}`);
        }

        // Appointment hints
        if (analysis.appointment?.rdvIntent) {
            parts.push('\nüìÖ RENDEZ-VOUS:');
            if (analysis.appointment.serviceType) parts.push(`- Service: ${analysis.appointment.serviceType}`);
            if (analysis.appointment.extractedSlots?.length > 0) {
                parts.push(`- Cr√©neaux d√©tect√©s: ${analysis.appointment.extractedSlots.join(', ')}`);
            }
        }

        // Human intervention needed
        if (analysis.needsHuman?.needed) {
            parts.push('\nüö® RECOMMANDATION: Propose de transf√©rer √† un humain');
            parts.push(`Raison: ${analysis.needsHuman.reasons.join(', ')}`);
        }
        
        return parts.join('\n');
    }

    /**
     * G√©n√®re une r√©ponse avec Google Gemini
     */
    async generateGeminiResponse(agent, conversationHistory, userMessage, knowledgeBase = [], messageAnalysis = null) {
        // Use centralized config for model mapping
        const geminiModel = getModelName('gemini', agent.model);

        console.log(`[AI] Using Gemini model: ${geminiModel}`);

        // Use centralized config for generation parameters
        const genConfig = getGenerationConfig(agent);
        // Min 2048 tokens pour √©viter la troncation (r√©ponses commande, r√©cap livraison, etc.)
        const maxOutputTokens = Math.max(genConfig.maxTokens || 500, 2048);
        const model = this.geminiClient.getGenerativeModel({ 
            model: geminiModel,
            generationConfig: {
                temperature: genConfig.temperature,
                maxOutputTokens,
            }
        });

        const { prompt: systemPrompt, promptHash } = this.buildSystemPrompt(agent, knowledgeBase, messageAnalysis);

        // Use smart conversation window for better context management
        let conversationText = '';
        const historyLimit = getHistoryLimit('gemini');
        const recentHistory = getSmartWindow(conversationHistory, {
            maxMessages: historyLimit,
            maxTokens: 1500,
            prioritizeRecent: true,
            enableCompression: conversationHistory.length > 20
        });
        
        if (recentHistory.length > 0) {
            conversationText = '\n\nüí¨ CONVERSATION R√âCENTE (contexte uniquement):\n';
            for (const msg of recentHistory) {
                const role = msg.role === 'user' ? 'üë§ Client' : 'ü§ñ Assistant';
                conversationText += `${role}: ${msg.content}\n`;
            }
        }

        const messageActuelLabel = '\n\n---\nüì© MESSAGE ACTUEL DU CLIENT (r√©ponds √† ce message en priorit√©):\n';
        const structuredInstruction = '\n\n‚ö†Ô∏è FORMAT DE R√âPONSE ‚Äî R√©ponds UNIQUEMENT par un objet JSON valide avec: "response" (string: ton message au client), "need_human" (boolean: true si tu recommandes de transf√©rer √† un humain), et optionnellement "confidence" (number 0-1). Exemple: {"response": "Bonjour, voici les informations...", "need_human": false, "confidence": 0.9}';
        const fullPrompt = `${systemPrompt}${conversationText}${messageActuelLabel}${userMessage}\n---\n\nü§ñ Assistant:${structuredInstruction}`;

        const result = await model.generateContent(fullPrompt);
        const rawResponse = result.response.text();

        // Use tokenizer for more accurate token counting
        const tokensUsed = countTokensSync(fullPrompt) + countTokensSync(rawResponse);
        const parsed = this.parseStructuredLlmResponse(rawResponse);
        const validated = this.validateStructuredOutput(parsed, 4096);

        console.log(`[AI] Gemini r√©ponse (${tokensUsed} tokens), need_human=${validated.need_human}: "${validated.content.substring(0, 50)}..."`);

        return {
            content: validated.content,
            need_human: validated.need_human,
            tokens: tokensUsed,
            provider: 'gemini',
            model: geminiModel,
            prompt_version: promptHash
        };
    }

    /**
     * Extract first top-level JSON object from string (brace-matching, respect strings).
     * @param {string} str - String starting with {
     * @returns {string | null}
     */
    _extractJsonObject(str) {
        if (!str || str[0] !== '{') return null;
        let depth = 0;
        let inString = false;
        let escape = false;
        let quote = '';
        for (let i = 0; i < str.length; i++) {
            const c = str[i];
            if (escape) {
                escape = false;
                continue;
            }
            if (inString) {
                if (c === '\\') escape = true;
                else if (c === quote) inString = false;
                continue;
            }
            if (c === '"' || c === "'") {
                inString = true;
                quote = c;
                continue;
            }
            if (c === '{') depth++;
            else if (c === '}') {
                depth--;
                if (depth === 0) return str.slice(0, i + 1);
            }
        }
        return null;
    }

    /**
     * Validate parsed LLM response schema
     * @param {any} obj - Object to validate
     * @returns {{ valid: boolean, errors: string[] }}
     */
    validateLlmResponseSchema(obj) {
        const errors = [];
        
        if (!obj || typeof obj !== 'object') {
            errors.push('Response must be an object');
            return { valid: false, errors };
        }
        
        if (!('response' in obj)) {
            errors.push('Missing required field: response');
        } else if (typeof obj.response !== 'string') {
            errors.push('Field "response" must be a string');
        } else if (obj.response.length === 0) {
            errors.push('Field "response" cannot be empty');
        }
        
        if ('need_human' in obj && typeof obj.need_human !== 'boolean') {
            errors.push('Field "need_human" must be a boolean');
        }
        
        if ('confidence' in obj) {
            if (typeof obj.confidence !== 'number') {
                errors.push('Field "confidence" must be a number');
            } else if (obj.confidence < 0 || obj.confidence > 1) {
                errors.push('Field "confidence" must be between 0 and 1');
            }
        }
        
        return { valid: errors.length === 0, errors };
    }

    /**
     * Parse LLM raw text as structured JSON { response, need_human, confidence? }.
     * Enhanced with schema validation
     * @param {string} raw
     * @returns {{ response: string, need_human: boolean, confidence?: number } | null}
     */
    parseStructuredLlmResponse(raw) {
        if (!raw || typeof raw !== 'string') {
            aiLogger.warn('Invalid LLM response: empty or not a string');
            return null;
        }
        
        const trimmed = raw.trim();
        
        const normalizeParsed = (obj) => ({
            response: obj.response,
            need_human: Boolean(obj.need_human ?? obj.need_confirmation),
            confidence: typeof obj.confidence === 'number' ? obj.confidence : undefined
        });

        // Try 1: Direct JSON parse
        try {
            const obj = JSON.parse(trimmed);
            const validation = this.validateLlmResponseSchema({ ...obj, need_human: obj.need_human ?? obj.need_confirmation });
            if (validation.valid) return normalizeParsed(obj);
            else aiLogger.warn('Invalid LLM response schema', { errors: validation.errors });
        } catch (error) {
            // Not valid JSON, try extracting
        }

        // Try 2: Extract JSON block from markdown or mixed content
        const jsonBlock = trimmed.match(/\{[\s\S]*"response"[\s\S]*\}/);
        if (jsonBlock) {
            try {
                const obj = JSON.parse(jsonBlock[0]);
                const validation = this.validateLlmResponseSchema({ ...obj, need_human: obj.need_human ?? obj.need_confirmation });
                if (validation.valid) {
                    aiLogger.info('Extracted JSON from mixed content');
                    return normalizeParsed(obj);
                } else aiLogger.warn('Extracted JSON has invalid schema', { errors: validation.errors });
            } catch (error) {
                aiLogger.warn('Failed to parse extracted JSON block', { error: error.message });
            }
        }
        
        // Try 3: Code block (```json ... ``` or ``` ... ```) - capture content then parse
        const codeBlockOpen = trimmed.match(/^```(?:json)?\s*\n?([\s\S]*)/);
        if (codeBlockOpen) {
            const blockContent = codeBlockOpen[1].split('```')[0].trim();
            const firstBrace = blockContent.indexOf('{');
            if (firstBrace !== -1) {
                const extracted = this._extractJsonObject(blockContent.slice(firstBrace));
                if (extracted) {
                    try {
                        const obj = JSON.parse(extracted);
                        const validation = this.validateLlmResponseSchema({ ...obj, need_human: obj.need_human ?? obj.need_confirmation });
                        if (validation.valid) {
                            aiLogger.info('Extracted JSON from code block');
                            return normalizeParsed(obj);
                        }
                    } catch (e) { /* continue */ }
                }
            }
        }

        // Try 4: First top-level JSON object by brace matching (robust when code block is unclosed)
        const firstBrace = trimmed.indexOf('{');
        if (firstBrace !== -1) {
            const extracted = this._extractJsonObject(trimmed.slice(firstBrace));
            if (extracted) {
                try {
                    const obj = JSON.parse(extracted);
                    const validation = this.validateLlmResponseSchema({ ...obj, need_human: obj.need_human ?? obj.need_confirmation });
                    if (validation.valid) {
                        aiLogger.info('Extracted JSON by brace matching');
                        return normalizeParsed(obj);
                    }
                } catch (e) { /* continue */ }
            }
        }

        // Try 5: Salvage truncated JSON - extract "response": "..." (handles escaped quotes, optional closing quote)
        const responseMatch = trimmed.match(/"response"\s*:\s*"((?:[^"\\]|\\.)*)"?/);
        if (responseMatch && responseMatch[1]?.trim()) {
            const text = responseMatch[1].replace(/\\"/g, '"').trim();
            if (text.length > 0) {
                aiLogger.info('Salvaged response from truncated JSON');
                return { response: text, need_human: true, confidence: undefined };
            }
        }

        aiLogger.error('Failed to parse LLM response as structured JSON', { 
            preview: trimmed.substring(0, 200) 
        });
        return null;
    }

    /** Forbidden phrases (business rule): block or force need_human */
    static getForbiddenPhrases() {
        return ['garanti', 'garantie absolue', 'promesse de d√©lai', 'je garantis', '100% garanti'];
    }

    /**
     * Validate and sanitize structured output; on failure force need_human and fallback text.
     * Applies max length, forbidden phrases, and low-confidence rule.
     * @param {{ response: string, need_human: boolean, confidence?: number } | null} parsed
     * @param {number} maxLength
     * @returns {{ content: string, need_human: boolean }}
     */
    validateStructuredOutput(parsed, maxLength = 4096) {
        const fallbackContent = 'Merci pour votre message. Un conseiller vous r√©pondra si n√©cessaire.';
        if (!parsed || typeof parsed.response !== 'string') {
            return { content: fallbackContent, need_human: true };
        }
        let content = parsed.response.trim();
        let need_human = Boolean(parsed.need_human);
        
        // Debug logging (only if enabled via environment variable)
        if (process.env.ENABLE_AGENT_DEBUG_LOGS === 'true') {
            this.logDebug('ai.js:validateStructuredOutput', 'Initial need_human from AI', {
                need_human_from_ai: parsed.need_human,
                confidence: parsed.confidence,
                content_preview: content.substring(0, 100)
            });
        }
        
        if (content.length > maxLength) content = content.slice(0, maxLength - 1) + '‚Ä¶';
        if (!content) content = fallbackContent;
        const lower = content.toLowerCase();
        for (const phrase of AIService.getForbiddenPhrases()) {
            if (lower.includes(phrase)) {
                if (process.env.ENABLE_AGENT_DEBUG_LOGS === 'true') {
                    this.logDebug('ai.js:validateStructuredOutput', 'need_human triggered by forbidden phrase', {
                        phrase,
                        content_preview: content.substring(0, 100)
                    });
                }
                need_human = true;
                break;
            }
        }
        if (typeof parsed.confidence === 'number' && parsed.confidence < 0.6) {
            if (process.env.ENABLE_AGENT_DEBUG_LOGS === 'true') {
                this.logDebug('ai.js:validateStructuredOutput', 'need_human triggered by low confidence', {
                    confidence: parsed.confidence,
                    was_need_human_before: need_human
                });
            }
            need_human = true;
        }
        
        if (process.env.ENABLE_AGENT_DEBUG_LOGS === 'true') {
            this.logDebug('ai.js:validateStructuredOutput', 'Final need_human value', {
                need_human,
                content_preview: content.substring(0, 100)
            });
        }
        
        return { content, need_human };
    }

    /**
     * G√©n√®re une r√©ponse √† partir d'une image (vision) - Gemini uniquement
     * @param {Object} agent - Agent configuration
     * @param {Array} conversationHistory - Previous messages
     * @param {string} imageBase64 - Image data (base64)
     * @param {string} mimeType - e.g. 'image/jpeg', 'image/png'
     * @param {string|null} caption - Optional caption from the user
     * @param {Array} knowledgeBase - Knowledge + products catalog
     * @param {string|null} userId - For credit deduction
     */
    /**
     * Resolve Gemini model id for vision/audio (images, notes vocales).
     * Uses agent.media_model if set, else agent.model. Only Gemini models are used for vision.
     */
    resolveMediaModel(agent) {
        const raw = agent?.media_model || agent?.model;
        if (!raw || typeof raw !== 'string') return 'gemini-1.5-flash-latest';
        const m = raw.toLowerCase();
        if (m.includes('gemini-1.5-pro') || m.includes('gemini-pro')) return 'gemini-1.5-pro-latest';
        if (m.includes('gemini-2.5') || m.includes('models/gemini-2.5-flash')) return 'gemini-2.5-flash';
        if (m.includes('gemini')) return 'gemini-1.5-flash-latest';
        return 'gemini-1.5-flash-latest';
    }

    /**
     * Transcrit un message audio (notes vocales) via Gemini.
     * @param {Object} agent - Agent configuration
     * @param {string} audioBase64 - Audio data (base64)
     * @param {string} mimeType - e.g. 'audio/ogg', 'audio/mpeg'
     * @param {string|null} userId - For credit deduction
     */
    async transcribeAudio(agent, audioBase64, mimeType, userId = null) {
        this.initialize();
        if (!this.geminiClient) {
            console.warn('[AI] Audio: Gemini non configur√©, transcription impossible');
            return { text: null, provider: 'fallback', model: null };
        }

        const geminiModel = this.resolveMediaModel(agent);
        if (userId && !hasEnoughCredits(userId, 'ai_message')) {
            return {
                text: null,
                credit_warning: 'Cr√©dits insuffisants',
                provider: 'gemini',
                model: geminiModel
            };
        }

        try {
            const model = this.geminiClient.getGenerativeModel({
                model: geminiModel,
                generationConfig: {
                    temperature: 0.1,
                    maxOutputTokens: 400,
                }
            });

            const instruction = "Transcris pr√©cis√©ment le message vocal. R√©ponds uniquement avec le texte, sans ajout ni commentaire.";
            const audioPart = {
                inlineData: {
                    data: audioBase64,
                    mimeType: mimeType || 'audio/ogg'
                }
            };

            const result = await model.generateContent([instruction, audioPart]);
            const response = result.response.text();
            const tokensUsed = countTokensSync(instruction) + countTokensSync(response || '') + 512;

            if (userId) {
                const deduction = deductCredits(userId, 'ai_message', 1, { agent_id: agent?.id, tokens: tokensUsed });
                return {
                    text: response,
                    tokens: tokensUsed,
                    provider: 'gemini',
                    model: geminiModel,
                    credits_deducted: deduction.cost,
                    credits_remaining: deduction.credits_remaining
                };
            }
            return {
                text: response,
                tokens: tokensUsed,
                provider: 'gemini',
                model: geminiModel
            };
        } catch (error) {
            console.error('[AI] Audio transcription erreur:', error.message);
            return { text: null, error: error.message, provider: 'gemini', model: geminiModel };
        }
    }

    async generateResponseFromImage(agent, conversationHistory, imageBase64, mimeType, caption, knowledgeBase = [], userId = null) {
        this.initialize();
        if (!this.geminiClient) {
            console.warn('[AI] Vision: Gemini non configur√©, fallback texte');
            return {
                content: "Je ne peux pas analyser les images pour le moment. D√©crivez-moi ce que vous cherchez ou envoyez un message texte.",
                tokens: 0,
                provider: 'fallback',
                model: null
            };
        }

        const geminiModel = this.resolveMediaModel(agent);
        if (userId && !hasEnoughCredits(userId, 'ai_message')) {
            return {
                content: "Cr√©dits insuffisants pour l'analyse d'image. R√©essayez plus tard.",
                credit_warning: 'Cr√©dits insuffisants',
                tokens: 0,
                provider: 'gemini',
                model: geminiModel
            };
        }

        try {
            const model = this.geminiClient.getGenerativeModel({
                model: geminiModel,
                generationConfig: {
                    temperature: agent?.temperature ?? 0.7,
                    maxOutputTokens: agent?.max_tokens ?? 500,
                }
            });

            const { prompt: systemPrompt } = this.buildSystemPrompt(agent, knowledgeBase, null);
            let conversationText = '';
            const recentHistory = conversationHistory.slice(-10);
            if (recentHistory.length > 0) {
                conversationText = '\n\nüí¨ CONVERSATION R√âCENTE:\n';
                for (const msg of recentHistory) {
                    const role = msg.role === 'user' ? 'üë§ Client' : 'ü§ñ Assistant';
                    conversationText += `${role}: ${msg.content}\n`;
                }
            }

            const imageInstruction = caption
                ? `Le client a envoy√© cette image avec le message: "${caption}". Analyse l'image en tenant compte de sa demande et r√©ponds de mani√®re utile (identification produit, prix, stock si pertinent).`
                : `Le client a envoy√© cette image sans texte. D√©cris ce que tu vois. Si cela ressemble √† un produit du catalogue, identifie le produit le plus proche et donne son nom, prix et disponibilit√©. Sinon, r√©ponds de mani√®re courtoise et utile.`;

            const textPart = `${systemPrompt}${conversationText}\n\nüì∑ MESSAGE IMAGE:\n${imageInstruction}\n\nü§ñ Assistant:`;

            const imagePart = {
                inlineData: {
                    data: imageBase64,
                    mimeType: mimeType || 'image/jpeg'
                }
            };

            const result = await model.generateContent([textPart, imagePart]);
            const response = result.response.text();
            // Use tokenizer for more accurate token counting (add extra for image tokens)
            const tokensUsed = countTokensSync(textPart) + countTokensSync(response || '') + 258; // ~258 tokens for image

            if (userId) {
                const deduction = deductCredits(userId, 'ai_message', 1, { agent_id: agent?.id, tokens: tokensUsed });
                return {
                    content: response,
                    tokens: tokensUsed,
                    provider: 'gemini',
                    model: geminiModel,
                    credits_deducted: deduction.cost,
                    credits_remaining: deduction.credits_remaining
                };
            }
            return {
                content: response,
                tokens: tokensUsed,
                provider: 'gemini',
                model: geminiModel
            };
        } catch (error) {
            console.error('[AI] Vision erreur:', error.message);
            return {
                content: "Je n'ai pas pu analyser cette image. Pouvez-vous d√©crire ce que vous cherchez en texte ?",
                tokens: 0,
                provider: 'fallback',
                model: null
            };
        }
    }

    /**
     * G√©n√®re une r√©ponse avec OpenAI
     */
    async generateOpenAIResponse(agent, conversationHistory, userMessage, knowledgeBase = [], messageAnalysis = null) {
        const { prompt: systemPrompt } = this.buildSystemPrompt(agent, knowledgeBase, messageAnalysis);
        const structuredInstruction = '\n\n‚ö†Ô∏è FORMAT DE R√âPONSE ‚Äî R√©ponds UNIQUEMENT par un objet JSON valide avec: "response" (string: ton message au client), "need_human" (boolean: true si tu recommandes de transf√©rer √† un humain), et optionnellement "confidence" (number 0-1). Exemple: {"response": "Bonjour, voici les informations...", "need_human": false, "confidence": 0.9}';

        // Construire les messages
        const messages = [
            { role: 'system', content: `${systemPrompt}${structuredInstruction}` }
        ];

        // Use smart conversation window for better context management
        const historyLimit = getHistoryLimit('openai');
        const recentHistory = getSmartWindow(conversationHistory, {
            maxMessages: historyLimit,
            maxTokens: 2000,
            prioritizeRecent: true,
            enableCompression: conversationHistory.length > 20
        });
        for (const msg of recentHistory) {
            messages.push({
                role: msg.role === 'user' ? 'user' : 'assistant',
                content: msg.content
            });
        }

        // Message actuel (explicite pour que le mod√®le sache √† quoi r√©pondre)
        messages.push({
            role: 'user',
            content: `üì© MESSAGE ACTUEL DU CLIENT (r√©ponds √† ce message):\n\n${userMessage}`
        });

        // Use centralized config for model mapping
        const openaiModel = getModelName('openai', agent.model);

        // Use centralized config for generation parameters
        const genConfig = getGenerationConfig(agent);
        const maxTokens = Math.max(genConfig.maxTokens || 500, 2048);
        const completion = await this.openaiClient.chat.completions.create({
            model: openaiModel,
            messages,
            temperature: genConfig.temperature,
            max_tokens: maxTokens
        });

        const rawResponse = completion.choices[0]?.message?.content || '';
        const tokensUsed = completion.usage?.total_tokens || 0;
        const parsed = this.parseStructuredLlmResponse(rawResponse);
        const validated = this.validateStructuredOutput(parsed, 4096);

        console.log(`[AI] OpenAI r√©ponse (${tokensUsed} tokens), need_human=${validated.need_human}: "${validated.content.substring(0, 50)}..."`);

        return {
            content: validated.content,
            need_human: validated.need_human,
            tokens: tokensUsed,
            provider: 'openai',
            model: openaiModel
        };
    }

    /**
     * G√©n√®re une r√©ponse avec OpenRouter (acc√®s √† plusieurs mod√®les)
     * Avec retry automatique et fallback vers d'autres mod√®les gratuits
     */
    async generateOpenRouterResponse(agent, conversationHistory, userMessage, knowledgeBase = [], messageAnalysis = null) {
        const { prompt: systemPrompt } = this.buildSystemPrompt(agent, knowledgeBase, messageAnalysis);
        const structuredInstruction = '\n\n‚ö†Ô∏è FORMAT DE R√âPONSE ‚Äî R√©ponds UNIQUEMENT par un objet JSON valide avec: "response" (string: ton message au client), "need_human" (boolean: true si tu recommandes de transf√©rer √† un humain), et optionnellement "confidence" (number 0-1). Exemple: {"response": "Bonjour, voici les informations...", "need_human": false, "confidence": 0.9}';

        // Construire les messages
        const messages = [
            { role: 'system', content: `${systemPrompt}${structuredInstruction}` }
        ];

        // Use smart conversation window for better context management
        const historyLimit = getHistoryLimit('openrouter');
        const recentHistory = getSmartWindow(conversationHistory, {
            maxMessages: historyLimit,
            maxTokens: 2000,
            prioritizeRecent: true,
            enableCompression: conversationHistory.length > 20
        });
        for (const msg of recentHistory) {
            messages.push({
                role: msg.role === 'user' ? 'user' : 'assistant',
                content: msg.content
            });
        }

        // Message actuel (explicite pour que le mod√®le sache √† quoi r√©pondre)
        messages.push({
            role: 'user',
            content: `üì© MESSAGE ACTUEL DU CLIENT (r√©ponds √† ce message):\n\n${userMessage}`
        });

        // Use centralized config for model selection (mapping agent.model -> ID OpenRouter)
        const primaryModel = getModelName('openrouter', agent.model || AI_CONFIG.models.openrouter.default);
        
        // Use centralized config for fallback models
        const fallbackModels = AI_CONFIG.models.openrouter.freeFallbacks
            .filter(m => m !== primaryModel);

        // Essayer le mod√®le principal d'abord, puis les fallbacks
        const modelsToTry = [primaryModel, ...fallbackModels];
        
        // Use centralized config for generation parameters
        const genConfig = getGenerationConfig(agent);
        const maxTokens = Math.max(genConfig.maxTokens || 500, 2048);
        
        for (let i = 0; i < modelsToTry.length; i++) {
            const modelName = modelsToTry[i];
            const isRetry = i > 0;
            
            try {
                if (isRetry) {
                    console.log(`[AI] Tentative avec mod√®le alternatif: ${modelName}`);
                } else {
                    console.log(`[AI] Using OpenRouter model: ${modelName}`);
                }

                const completion = await this.openrouterClient.chat.completions.create({
                    model: modelName,
                    messages,
                    temperature: genConfig.temperature,
                    max_tokens: maxTokens
                });

                let rawResponse = completion.choices[0]?.message?.content || '';
                const tokensUsed = completion.usage?.total_tokens || 0;

                let parsed = this.parseStructuredLlmResponse(rawResponse);
                if (!parsed) {
                    const cleaned = this.cleanReasoningResponse(rawResponse);
                    parsed = this.parseStructuredLlmResponse(cleaned);
                }
                const validated = this.validateStructuredOutput(parsed, 4096);

                console.log(`[AI] OpenRouter r√©ponse (${tokensUsed} tokens), need_human=${validated.need_human}: "${validated.content.substring(0, 50)}..."`);

                return {
                    content: validated.content,
                    need_human: validated.need_human,
                    tokens: tokensUsed,
                    provider: 'openrouter',
                    model: modelName
                };
                
            } catch (error) {
                const is429 = error.status === 429 || error.message?.includes('429');
                const isLastModel = i === modelsToTry.length - 1;
                
                console.error(`[AI] Erreur openrouter (${modelName}): ${error.status || ''} ${error.message?.substring(0, 50)}`);
                
                // Log rate limit for admin
                if (is429) {
                    try {
                        const userId = agent.user_id;
                        adminAnomaliesService.logRateLimit(userId, modelName);
                    } catch (e) { /* ignore */ }
                }
                
                if (is429 && !isLastModel) {
                    // Rate limit - attendre un peu avant d'essayer le mod√®le suivant
                    const waitTime = 1000 * (i + 1); // 1s, 2s, 3s...
                    console.log(`[AI] Rate limit atteint, attente ${waitTime/1000}s avant mod√®le suivant...`);
                    await new Promise(resolve => setTimeout(resolve, waitTime));
                    continue;
                }
                
                if (isLastModel) {
                    // Dernier mod√®le aussi √©chou√©, propager l'erreur pour fallback Gemini
                    throw error;
                }
            }
        }
        
        // Ne devrait jamais arriver, mais au cas o√π
        throw new Error('Tous les mod√®les OpenRouter ont √©chou√©');
    }

    /**
     * Nettoie les r√©ponses des mod√®les de raisonnement (supprime la partie "thinking")
     * Optimis√© pour DeepSeek R1 et autres mod√®les qui exposent leur r√©flexion
     */
    cleanReasoningResponse(response) {
        if (!response) return response;

        let cleaned = response;

        // 1. Supprimer les balises <think>...</think> (format standard DeepSeek R1)
        cleaned = cleaned.replace(/<think>[\s\S]*?<\/think>/gi, '').trim();

        // 2. Patterns de salutations/d√©but de vraie r√©ponse (FR + EN)
        const greetingPatterns = [
            /^(Bonjour|Salut|Hello|Hi|Hey|Bienvenue|Bonsoir|Coucou|Cher|Ch√®re)/i,
            /^(Merci|Thank|Thanks|Je vous|Je te|Avec plaisir|Bien s√ªr|Certainement|Absolument)/i,
            /^(Nos|Notre|Votre|Vos|Le|La|Les|Un|Une|Pour|Voici|Concernant)/i,
            /^[üëãüòäüôåüí¨‚ú®üéØüìã]/,  // Commence par emoji
        ];

        // 3. Patterns de thinking √† supprimer (EN principalement car DeepSeek pense en anglais)
        const thinkingStartPatterns = [
            /^(Okay|Ok|Alright|Right|Well|So|Now|First|Let me|I need|I should|I'll|I will|I can|I have|I want)/i,
            /^(The user|This user|They|He|She|Looking|Considering|Given|Since|Based|According)/i,
            /^(That should|This should|That covers|This covers|That's|This is|It's|Here's what)/i,
            /^(Hmm|Hm|Um|Uh|Let's|Got it|Sure|Yeah|Yes|No problem)/i,
            /^(My response|My answer|I think|I believe|I understand|I see|I notice)/i,
            /^(Friendly|Professional|Clear|Concise|Brief|Short|Simple)/i,
            /^(Step|Point|Note|Remember|Keep|Make sure|Don't forget)/i,
        ];

        // 4. Chercher la vraie r√©ponse en scannant ligne par ligne
        const lines = cleaned.split('\n');
        let realContentStartIndex = -1;

        for (let i = 0; i < lines.length; i++) {
            const line = lines[i].trim();
            if (!line) continue;

            // V√©rifier si c'est une ligne de greeting/vraie r√©ponse
            const isGreeting = greetingPatterns.some(pattern => pattern.test(line));
            if (isGreeting) {
                realContentStartIndex = i;
                break;
            }

            // V√©rifier si c'est du thinking
            const isThinking = thinkingStartPatterns.some(pattern => pattern.test(line));
            if (!isThinking) {
                // Ce n'est ni un greeting ni du thinking √©vident
                // V√©rifier si √ßa ressemble √† du contenu (pas en anglais si contexte FR)
                const looksLikeThinking = line.match(/(should|would|could|need to|have to|going to|want to|the user|their|they)/i);
                if (!looksLikeThinking) {
                    realContentStartIndex = i;
                    break;
                }
            }
        }

        // Si on a trouv√© le d√©but du vrai contenu
        if (realContentStartIndex > 0) {
            cleaned = lines.slice(realContentStartIndex).join('\n').trim();
        }

        // 5. Nettoyage final par paragraphes
        const paragraphs = cleaned.split('\n\n');
        const filteredParagraphs = paragraphs.filter(p => {
            const trimmed = p.trim().toLowerCase();
            
            // Exclure les paragraphes qui sont clairement du thinking
            const thinkingPhrases = [
                'okay,', 'alright,', 'let me ', 'i need to ', 'i should ', 
                'the user ', 'i\'ll ', 'i will ', 'that should ', 'this should ',
                'that covers', 'friendly,', 'professional,', 'my response',
                'here\'s what', 'looking at', 'considering', 'based on',
                'step 1', 'first,', 'now,', 'so,', 'well,'
            ];
            
            const isThinkingParagraph = thinkingPhrases.some(phrase => 
                trimmed.startsWith(phrase) || trimmed.includes(phrase)
            );

            // Garder si ce n'est pas du thinking
            return !isThinkingParagraph;
        });

        if (filteredParagraphs.length > 0) {
            cleaned = filteredParagraphs.join('\n\n').trim();
        }

        // 6. Dernier recours: chercher apr√®s un pattern de fin de thinking
        if (cleaned.match(/^(Okay|Alright|That|This|So|Now|First|Let)/i)) {
            // Chercher la premi√®re vraie phrase FR ou salutation
            const match = cleaned.match(/(Bonjour|Salut|Hello|Hi|Merci|Nos |Notre |Votre |Pour |Voici |üëã|üòä)[^]*/i);
            if (match) {
                cleaned = match[0].trim();
            }
        }

        // Log pour debug
        if (cleaned !== response) {
            console.log(`[AI] Cleaned thinking from response. Original: ${response.length} chars, Cleaned: ${cleaned.length} chars`);
        }

        return cleaned || response;
    }

    /**
     * R√©ponses de fallback quand aucune IA n'est disponible
     */
    fallbackResponse(agent, userMessage) {
        const lowerMessage = userMessage.toLowerCase().trim();
        
        // R√©ponses intelligentes bas√©es sur des mots-cl√©s
        const responses = {
            // Salutations
            'bonjour': `Bonjour ! üëã Je suis ${agent.name || 'votre assistant'}. Comment puis-je vous aider aujourd'hui ?`,
            'salut': `Salut ! üòä Comment puis-je vous aider ?`,
            'hello': `Hello! üëã How can I help you today?`,
            'hi': `Hi there! üëã How can I help you?`,
            'hey': `Hey ! üëã Comment √ßa va ?`,
            'coucou': `Coucou ! üòä Que puis-je faire pour vous ?`,
            
            // Remerciements
            'merci': `Je vous en prie ! üòä N'h√©sitez pas si vous avez d'autres questions.`,
            'thanks': `You're welcome! üòä Feel free to ask if you need anything else.`,
            'thank you': `You're welcome! üòä`,
            
            // Demandes d'aide
            'aide': `Je suis l√† pour vous aider ! üôå Posez-moi vos questions.`,
            'help': `I'm here to help! üôå What do you need?`,
            
            // Questions fr√©quentes
            'prix': `Pour conna√Ætre nos tarifs, je vous invite √† consulter notre site web ou √† nous contacter directement. üí∞`,
            'tarif': `Pour les tarifs, veuillez nous contacter ou visiter notre site. üí∞`,
            'horaire': `Nos horaires sont disponibles sur notre site web. Notre assistant est disponible 24/7 ! ‚è∞`,
            'contact': `Vous pouvez nous contacter directement ici sur WhatsApp ! üì±`,
            'adresse': `Pour notre adresse, veuillez consulter notre site web ou nous contacter. üìç`,
            
            // Au revoir
            'bye': `Au revoir ! üëã √Ä bient√¥t !`,
            'au revoir': `Au revoir ! üëã N'h√©sitez pas √† revenir si vous avez des questions.`,
            'bonne journ√©e': `Merci, bonne journ√©e √† vous aussi ! ‚òÄÔ∏è`,
            
            // Oui/Non
            'oui': `Parfait ! üëç Comment puis-je vous aider ?`,
            'non': `D'accord. Y a-t-il autre chose que je puisse faire pour vous ?`,
            'ok': `Super ! üëç`,
        };

        // Chercher une correspondance
        for (const [key, response] of Object.entries(responses)) {
            if (lowerMessage.includes(key) || lowerMessage === key) {
                return { content: response, tokens: 0, provider: 'fallback' };
            }
        }

        // R√©ponse par d√©faut
        return {
            content: `Merci pour votre message ! üòä Je suis ${agent.name || 'votre assistant'}. Notre √©quipe vous r√©pondra tr√®s bient√¥t. En attendant, n'h√©sitez pas √† me poser d'autres questions !`,
            tokens: 0,
            provider: 'fallback'
        };
    }

    /**
     * Debug logging helper (only executes if ENABLE_AGENT_DEBUG_LOGS=true)
     * Sends logs to external debug endpoint
     */
    logDebug(location, message, data) {
        if (process.env.ENABLE_AGENT_DEBUG_LOGS !== 'true') return;
        
        try {
            debugIngest({
                location,
                message,
                data,
                timestamp: Date.now(),
                sessionId: 'debug-session',
                hypothesisId: 'H1'
            });
        } catch (error) {
            // Ignore any errors to prevent debug logging from breaking the app
        }
    }
}

export const aiService = new AIService();
export default aiService;
